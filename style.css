To ensure Red Hat has the full picture, we should clarify that this OOM issue began after:

The latest custom plugin deployments

UI revamp

Enabling the orchestrator in RHDH

This was not occurring in RHDH v1.5.1. Providing this context will help Red Hat focus their investigation appropriately.

Letâ€™s share these details with them immediately.



node -e "
const used = process.memoryUsage();
console.log('Memory Usage:');
Object.keys(used).forEach(key => {
  console.log(\`  \${key}: \${Math.round(used[key] / 1024 / 1024)} MB\`);
});
console.log('RSS (Total):', Math.round(used.rss / 1024 / 1024) + 'MB');
"



node --inspect -e "setTimeout(() => {}, 60000)"




1. First, Enable Debugging in Your Pod
bash
# Start the debug session
oc exec <pod-name> -n <namespace> -- node --inspect -e "setTimeout(() => {}, 60000)" &
Note: The & runs it in background so you can run other commands.

2. Port Forward to Access the Debugger
bash
# Find the debug port (usually 9229)
oc port-forward <pod-name> -n <namespace> 9229:9229
3. Connect Chrome DevTools
Open Chrome and go to chrome://inspect

Click "Configure..." and add localhost:9229

You should see your remote target appear under "Remote Target"

Click "inspect" to open DevTools


You're absolutely right! Let me clarify the strategy:

The first step with process.memoryUsage() was just to identify which of your 3 pods is the most critical - the one with the worst memory performance that needs deep investigation.

Now that you've identified the problematic pod, the next step with Node Inspector + Chrome DevTools is exactly what you should run on that specific pod to get detailed heap analysis and find the root cause of the memory issues.
