To move forward with Red Hat support, we must provide heap dump analysis. This is critical to determine whether the OOM issue originates in our custom plugins or the core product.

I’ve shared the instructions for generating Node.js memory dumps. Please prioritize this so we can supply Red Hat with the necessary data and resolve the issue efficiently.

Let me know if there are any blockers—I’m happy to help.



node -e "
const used = process.memoryUsage();
console.log('Memory Usage:');
Object.keys(used).forEach(key => {
  console.log(\`  \${key}: \${Math.round(used[key] / 1024 / 1024)} MB\`);
});
console.log('RSS (Total):', Math.round(used.rss / 1024 / 1024) + 'MB');
"



node --inspect -e "setTimeout(() => {}, 60000)"




1. First, Enable Debugging in Your Pod
bash
# Start the debug session
oc exec <pod-name> -n <namespace> -- node --inspect -e "setTimeout(() => {}, 60000)" &
Note: The & runs it in background so you can run other commands.

2. Port Forward to Access the Debugger
bash
# Find the debug port (usually 9229)
oc port-forward <pod-name> -n <namespace> 9229:9229
3. Connect Chrome DevTools
Open Chrome and go to chrome://inspect

Click "Configure..." and add localhost:9229

You should see your remote target appear under "Remote Target"

Click "inspect" to open DevTools


You're absolutely right! Let me clarify the strategy:

The first step with process.memoryUsage() was just to identify which of your 3 pods is the most critical - the one with the worst memory performance that needs deep investigation.

Now that you've identified the problematic pod, the next step with Node Inspector + Chrome DevTools is exactly what you should run on that specific pod to get detailed heap analysis and find the root cause of the memory issues.
