
Team Update - Memory Leak Issue:

âœ… Temporary fix working: Increased memory to 4Gi is buying us time - OOM kills now happen every 2 days instead of daily

ðŸ”„ Action plan:

Off-hours restart: Setting up periodic rollout restarts during low traffic

QA replication: Created Jira subtask to reproduce the memory leak in QA environment

Alert check: @Bippin - can you verify which distribution list gets the OOM alerts? I didn't receive any notifications



SELECT 
    CASE 
        WHEN metadata->>'name' LIKE '11840_%' THEN 'service_account'
        WHEN metadata->>'name' LIKE '%sa' THEN 'support_user' 
        ELSE 'developer_user'
    END as user_type,
    COUNT(*) as count
FROM cds.user_entities
GROUP BY user_type;




Task ID	Cadence	Meaning	Purpose
run_ldap_user_refresh	PT6H	Every 6 Hours	Refreshes user data from LDAP/Active Directory
run_ldap_group_refresh	PT6H	Every 6 Hours	Refreshes group memberships from LDAP/AD
run_project_refresh	0 */4 * * *	Every 4 Hours (Cron syntax)	General catalog processing/entity refresh
run_csi_refresh	0 0 * * *	Daily at Midnight (Cron)	Custom Software Integration refresh
run_cluster_refresh	0 0 * * *	Daily at Midnight (Cron)	Your OpenShift/Kubernetes cluster refresh
run_update_request_status...	PT30M	Every 30 Minutes	Internal order management status updates




-- Entities with processing errors that haven't been updated in a long time
SELECT COUNT(*) 
FROM refresh_state 
WHERE errors IS NOT NULL 
  AND next_update_at < NOW() - INTERVAL '30 days'
  AND last_discovery_at < NOW() - INTERVAL '30 days';-- Find entities that haven't been attempted in a long time and are not scheduled for a future update.
SELECT COUNT(*), entity_id, status
FROM refresh_state
WHERE next_update_at < NOW() - INTERVAL '7 days'
  AND (last_discovery_at < NOW() - INTERVAL '7 days' OR last_discovery_at IS NULL)
GROUP BY entity_id, status;




-- DELETE entities with errors that are very old and not being processed
DELETE FROM refresh_state 
WHERE errors IS NOT NULL 
  AND next_update_at < NOW() - INTERVAL '30 days'
  AND last_discovery_at < NOW() - INTERVAL '30 days'
LIMIT 5000; -- Start with a small batch
